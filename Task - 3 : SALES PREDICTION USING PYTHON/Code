Task--
.Sales prediction involves forecasting the amount of a product that customers will purchase, taking into account various factors such as advertising expenditure, target audience segmentation, and advertising platform selection.

.In businesses that offer products or services, the role of a Data Scientist is crucial for predicting future sales. They utilize machine learning techniques in Python to analyze and interpret data, allowing them to make informed decisions regarding advertising costs. By leveraging these predictions, businesses can optimize their advertising strategies and maximize sales potential. Let's embark on the journey of sales prediction using machine learning in Python.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
​
​
from statsmodels.stats.outliers_influence import variance_inflation_factor
​
from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score
​
import warnings
warnings.filterwarnings('ignore')
 
df=pd.read_csv("advertising.csv")
print (df)
        TV  Radio  Newspaper  Sales
0    230.1   37.8       69.2   22.1
1     44.5   39.3       45.1   10.4
2     17.2   45.9       69.3   12.0
3    151.5   41.3       58.5   16.5
4    180.8   10.8       58.4   17.9
..     ...    ...        ...    ...
195   38.2    3.7       13.8    7.6
196   94.2    4.9        8.1   14.0
197  177.0    9.3        6.4   14.8
198  283.6   42.0       66.2   25.5
199  232.1    8.6        8.7   18.4

[200 rows x 4 columns]

df.head()
TV	Radio	Newspaper	Sales
0	230.1	37.8	69.2	22.1
1	44.5	39.3	45.1	10.4
2	17.2	45.9	69.3	12.0
3	151.5	41.3	58.5	16.5
4	180.8	10.8	58.4	17.9
df.columns
Index(['TV', 'Radio', 'Newspaper', 'Sales'], dtype='object')

df.describe()
TV	Radio	Newspaper	Sales
count	200.000000	200.000000	200.000000	200.000000
mean	147.042500	23.264000	30.554000	15.130500
std	85.854236	14.846809	21.778621	5.283892
min	0.700000	0.000000	0.300000	1.600000
25%	74.375000	9.975000	12.750000	11.000000
50%	149.750000	22.900000	25.750000	16.000000
75%	218.825000	36.525000	45.100000	19.050000
max	296.400000	49.600000	114.000000	27.000000

df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 200 entries, 0 to 199
Data columns (total 4 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   TV         200 non-null    float64
 1   Radio      200 non-null    float64
 2   Newspaper  200 non-null    float64
 3   Sales      200 non-null    float64
dtypes: float64(4)
memory usage: 6.4 KB

df.shape
(200, 4)

Data Cleaning

df.duplicated().sum()
0
df.isnull().sum()*100/df.shape[0]
TV           0.0
Radio        0.0
Newspaper    0.0
Sales        0.0
dtype: float64
sns.kdeplot(df["Radio"], fill = True)
<Axes: xlabel='Radio', ylabel='Density'>

df.head()
TV	Radio	Newspaper	Sales
0	230.1	37.8	69.2	22.1
1	44.5	39.3	45.1	10.4
2	17.2	45.9	69.3	12.0
3	151.5	41.3	58.5	16.5
4	180.8	10.8	58.4	17.9
​
EDA (Exploratory data Analysis)
sns.set()
df['TV'].hist()
<Axes: >

df['Newspaper'].hist()
<Axes: >

df['Radio'].hist()
​
<Axes: >

# Outlier Analysis
fig, axs = plt.subplots(3, figsize = (5,5))
plt1 = sns.boxplot(df['TV'], ax = axs[0])
plt2 = sns.boxplot(df['Newspaper'], ax = axs[1])
plt3 = sns.boxplot(df['Radio'], ax = axs[2])
plt.tight_layout()

Sales (Target Variable)
sns.boxplot(df['Sales'])
plt.show()

# Let's see how Sales are related with other variables using scatter plot.
sns.pairplot(df, x_vars=['TV', 'Newspaper', 'Radio'], y_vars='Sales', height=4, aspect=1, kind='scatter')
plt.show()

df.corr()
TV	Radio	Newspaper	Sales
TV	1.000000	0.054809	0.056648	0.901208
Radio	0.054809	1.000000	0.354104	0.349631
Newspaper	0.056648	0.354104	1.000000	0.157960
Sales	0.901208	0.349631	0.157960	1.000000
# Let's see the correlation between different variables.
sns.heatmap(df.corr(),annot=True,cmap='coolwarm')
<Axes: >

Assumption 2.No multicolinearity
df1 = df.drop(["Sales"],axis =1) 
df1.head()
df1.shape
(200, 3)
vif = []
for i in range(df1.shape[1]):
    vif_cal = variance_inflation_factor(df1.to_numpy(),i)
    vif.append(vif_cal)
vif
[2.486771835198193, 3.2854621001628947, 3.0552445106573844]
​
Feature Engineering
To find outliers
​
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3-Q1
​
lower_tail = Q1- 1.5 * IQR
upper_tail = Q3+ 1.5 * IQR
​
outliers = (df < lower_tail) | (df > upper_tail)
outliers_count = outliers.sum()
outliers_count
TV           0
Radio        0
Newspaper    2
Sales        0
dtype: int64
To replace outliers
​
def Finding_outliar1(df,columns):
    for column in columns:
            q1 = df[column].quantile(0.25)
            q3 = df[column].quantile(0.75)
            iqr = q3 - q1
​
            lower_tail1 = q1 - 1.5 * iqr
            upper_tail1 = q3 + 1.5 * iqr
            print(lower_tail1)
            print(upper_tail1)
            
            df[column] = df[column].apply(lambda x : upper_tail1 if x > upper_tail1 else (lower_tail1 if x < lower_tail1 else x))
columns = ["Newspaper"]
Finding_outliar1(df,columns)
-35.775000000000006
93.625
Model Building
Performing Simple Linear Regression
Equation of linear regression
y=c+m1x1+m2x2+...+mnxn

y is the response

c is the intercept

m1 is the coefficient for the first feature

mn is the coefficient for the nth feature

In our case:
y=c+m1×TV

The m values are called the model coefficients or model parameters

Generic Steps in model building using statsmodels
We first assign the feature variable, TV, in this case, to the variable X and the response variable, Sales, to the variable y.

X = df['TV']
y = df['Sales']
​
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)
X_train.head()
74     213.4
3      151.5
185    205.0
26     142.9
90     134.3
Name: TV, dtype: float64
y_train.head()
74     17.0
3      16.5
185    22.6
26     15.0
90     14.0
Name: Sales, dtype: float64
​
import statsmodels.api as sm
# Add a constant to get an intercept
X_train_sm = sm.add_constant(X_train)
​
# Fit the resgression line using 'OLS'
lr = sm.OLS(y_train, X_train_sm).fit()
# Print the parameters, i.e. the intercept and the slope of the regression line fitted
lr.params
const    6.948683
TV       0.054546
dtype: float64
# Performing a summary operation lists out all the different parameters of the regression line fitted
print(lr.summary())
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  Sales   R-squared:                       0.816
Model:                            OLS   Adj. R-squared:                  0.814
Method:                 Least Squares   F-statistic:                     611.2
Date:                Wed, 08 Jan 2025   Prob (F-statistic):           1.52e-52
Time:                        15:39:35   Log-Likelihood:                -321.12
No. Observations:                 140   AIC:                             646.2
Df Residuals:                     138   BIC:                             652.1
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          6.9487      0.385     18.068      0.000       6.188       7.709
TV             0.0545      0.002     24.722      0.000       0.050       0.059
==============================================================================
Omnibus:                        0.027   Durbin-Watson:                   2.196
Prob(Omnibus):                  0.987   Jarque-Bera (JB):                0.150
Skew:                          -0.006   Prob(JB):                        0.928
Kurtosis:                       2.840   Cond. No.                         328.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
F statistic has a very low p value (practically low)
Meaning that the model fit is statistically significant, and the explained variance isn't purely by chance.

The fit is significant. Let's visualize how well the model fit the data.

From the parameters that we get, our linear regression equation becomes:

Sales=6.948+0.054×TV

plt.scatter(X_train, y_train)
plt.plot(X_train, 6.948 + 0.054*X_train, 'r')
plt.show()

​
Model Evaluation
y_train_pred = lr.predict(X_train_sm)
res = (y_train - y_train_pred)
​
fig = plt.figure()
sns.distplot(res, bins = 15)
fig.suptitle('Error Terms', fontsize = 15)                  # Plot heading 
plt.xlabel('y_train - y_train_pred', fontsize = 15)         # X-label
plt.show()
C:\Users\debar\AppData\Local\Temp\ipykernel_11168\3003513444.py:2: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(res, bins = 15)

plt.scatter(X_train,res)
plt.show()

​
Predictions on the Test Set
# Add a constant to X_test
X_test_sm = sm.add_constant(X_test)
​
# Predict the y values corresponding to X_test_sm
y_pred = lr.predict(X_test_sm)
y_pred.head()
126     7.374140
104    19.941482
99     14.323269
92     18.823294
111    20.132392
dtype: float64
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
#Returns the mean squared error; we'll take a square root
np.sqrt(mean_squared_error(y_test, y_pred))
2.019296008966233
Checking the R-squared on the test set
r_squared = r2_score(y_test, y_pred)
r_squared
0.7921031601245657
plt.scatter(X_test, y_test)
plt.plot(X_test, 6.948 + 0.054 * X_test, 'r')
plt.show()

​
